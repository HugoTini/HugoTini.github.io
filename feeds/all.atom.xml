<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>HugoTini</title><link href="https://hugotini.github.io/" rel="alternate"></link><link href="https://hugotini.github.io/feeds/all.atom.xml" rel="self"></link><id>https://hugotini.github.io/</id><updated>2021-07-05T00:00:00+02:00</updated><entry><title>Gym Godot</title><link href="https://hugotini.github.io/gymgodot.html" rel="alternate"></link><published>2021-07-05T00:00:00+02:00</published><updated>2021-07-05T00:00:00+02:00</updated><author><name>HugoTini</name></author><id>tag:hugotini.github.io,2021-07-05:/gymgodot.html</id><summary type="html">&lt;p&gt;Reinforcement Learning &amp;amp; Godot&lt;/p&gt;</summary><content type="html">&lt;p&gt;For a side project, I had to experiment with Reinforcement Learning in &lt;a href="https://godotengine.org/"&gt;Godot&lt;/a&gt;. Some of the parts which might be useful have been extracted and are now available &lt;a href="https://github.com/HugoTini/GymGodot"&gt;in this repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The overall idea is that a Godot scene is driven through WebSocket by a Python-side server which is exposed as a standard &lt;a href="https://github.com/openai/gym"&gt;Gym&lt;/a&gt; environment (while not the fastest IPC solution, WebSocket is available by default in Godot which makes it easier to install than extra native modules or dependencies).&lt;/p&gt;
&lt;p&gt;To illustrate how to use those scripts, three demo environments were added, the classic Cartpole &amp;amp; Pendulum :&lt;/p&gt;
&lt;div class='fit_width img_container'&gt;
&lt;video controls&gt;
    &lt;source src='https://hugotini.github.io/assets/gymgodot/cartpole_pendulum.webm'
            type="video/webm"&gt;
&lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;and the Mars-Lander environment (a 3D take on the classic &lt;a href="https://gym.openai.com/envs/LunarLander-v2/"&gt;2D Lunar-Lander&lt;/a&gt;) :&lt;/p&gt;
&lt;div class='fit_width img_container'&gt;
&lt;video controls&gt;
    &lt;source src='https://hugotini.github.io/assets/gymgodot/mars_lander.webm'
            type="video/webm"&gt;
&lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;The physic &amp;amp; rendering happen step-by-step and offline so that the computation can go faster than real-time. Check &lt;a href="https://github.com/HugoTini/GymGodot"&gt;the repo&lt;/a&gt; for more details.&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>Normals to height</title><link href="https://hugotini.github.io/normalheight.html" rel="alternate"></link><published>2020-05-25T00:00:00+02:00</published><updated>2020-05-25T00:00:00+02:00</updated><author><name>HugoTini</name></author><id>tag:hugotini.github.io,2020-05-25:/normalheight.html</id><summary type="html">&lt;p&gt;Computing depth from gradients&lt;/p&gt;</summary><content type="html">&lt;p&gt;Previously, I wrote about &lt;a href="https://hugotini.github.io/deepbump.html"&gt;DeepBump&lt;/a&gt;, a machine 
learning experiment to generate normal maps from single pictures. However, 
when creating 3D (or 2D) content, the actual height ("displacement") 
of the surface might also be useful. To complement DeepBump, &lt;a href="https://github.com/HugoTini/NormalHeight"&gt;NormalHeight&lt;/a&gt; is small extra Blender add-on that computes the height from normals.&lt;/p&gt;
&lt;h1&gt;Depth-from-gradient&lt;/h1&gt;
&lt;p&gt;Going from a gradient (normal map) to the depth (height map) is
often called "shape-from-gradient" or "depth-from-gradient". This has typically
been used in computer vision, for instance after having found the normal map
of an object through &lt;a href="https://en.wikipedia.org/wiki/Photometric_stereo"&gt;photometric stereo&lt;/a&gt;. In our case, DeepBump already takes care of generating the normal map from a single photo. NormalHeight then uses the &lt;a href="https://webdocs.cs.ualberta.ca/~vis/courses/CompVis/readings/photometric/FrankotIntegrpami88.pdf"&gt;Frankot-Chellappa algorithm&lt;/a&gt; (Eq. 22) which uses the Fourier Domain to compute a height based on normals.&lt;/p&gt;
&lt;h1&gt;Usage&lt;/h1&gt;
&lt;p&gt;By combining DeepBump with NormalHeight, we can now get a displacement from a single photo :&lt;/p&gt;
&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/normalheight/compares.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/normalheight/compares.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;original photos : &lt;a href='https://unsplash.com/photos/wtjrpjZABcQ'&gt;a&lt;/a&gt;, &lt;a href='https://unsplash.com/photos/lXVbqVIImZY'&gt;b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Displacement can also be used for material mixing by thresholding the generated height (3rd picture) and using it as a mask :&lt;/p&gt;
&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/normalheight/mixing.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/normalheight/mixing.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;original textures : &lt;a href='https://opengameart.org/content/2048-digitally-painted-tileable-desert-sand-texture'&gt;a&lt;/a&gt;, &lt;a href='https://opengameart.org/content/handpainted-stone-wall-textures'&gt;b&lt;/a&gt;&lt;/p&gt;

&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/normalheight/mixing2.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/normalheight/mixing2.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/KCq781yFwWM'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Limitations&lt;/h1&gt;
&lt;p&gt;DeepBump learned to create locally probable normals, however, as the texture is
processed tile by tile, on large pictures the normals might lack general structure
(and thus the height map too).&lt;/p&gt;
&lt;p&gt;To go further, we could get better results by giving the neural net more context about
the whole picture rather than processing tiles independently. Moreover, we might get
better height maps by teaching the neural net to predict directly the depth rather than
the normals.&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>DeepBump</title><link href="https://hugotini.github.io/deepbump.html" rel="alternate"></link><published>2020-05-07T00:00:00+02:00</published><updated>2020-05-07T00:00:00+02:00</updated><author><name>HugoTini</name></author><id>tag:hugotini.github.io,2020-05-07:/deepbump.html</id><summary type="html">&lt;p&gt;Normal Map generation using Machine Learning&lt;/p&gt;</summary><content type="html">&lt;p&gt;Normal maps encode surfaces orientation, they're used a lot in computer graphics. They allow "faking" details without having to use extra geometry. Artists typically obtain normal maps in various ways : they might sculpt detailed geometry, then project it on a lower resolution mesh (retopology), they might generate materials procedurally (i.e. using math functions), use photogrammetry, etc.&lt;/p&gt;
&lt;p&gt;Those techniques require some manual efforts. Sometimes you just want to quickly &lt;a href="https://youtu.be/v_ikG-u_6r0"&gt;grab that photo and use it in your scene&lt;/a&gt;. Then it makes sense to try to generate normal maps from single pictures.&lt;/p&gt;
&lt;p&gt;A simple approach is to use the grayscale image as height from which you get the normal map (by taking the slope's normal). It can give decent results on some textures, but still is a rough approximation as darker pixels don't always mean there's a hole on the surface.&lt;/p&gt;
&lt;h1&gt;DeepBump&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/HugoTini/DeepBump"&gt;DeepBump&lt;/a&gt; is an experiment using machine learning for reconstructing normals from single pictures. It mainly makes use of an encoder-decoder &lt;a href="https://arxiv.org/abs/1505.04597"&gt;U-Net&lt;/a&gt; with a &lt;a href="https://arxiv.org/abs/1801.04381"&gt;MobileNetV2&lt;/a&gt; architecture. Training data is taken from real-world photogrammetry and procedural materials.&lt;/p&gt;
&lt;p&gt;This tool is available both as a Blender add-on and as a command-line program. Check &lt;a href="https://github.com/HugoTini/DeepBump"&gt;here&lt;/a&gt; for install instructions. Once installed, you can generate a normal map from a picture (&lt;em&gt;image texture node&lt;/em&gt; in Blender) in one click :&lt;/p&gt;
&lt;video controls&gt;
    &lt;source src='https://hugotini.github.io/assets/deepbump/addon_vid.webm'
            type="video/webm"&gt;
&lt;/video&gt;

&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;Here are a few shots of normal maps obtained using DeepBump (middle) and compared with the simple "grayscale = height" method (right):&lt;/p&gt;
&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/deepbump/compare1.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/deepbump/compare1.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/t4DhcQddCnA'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/deepbump/compare3.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/deepbump/compare3.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/uDdoiaWiKFA'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/deepbump/compare4.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/deepbump/compare4.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/4UjcOpLLQSE'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/deepbump/compare2.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/deepbump/compare2.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/xnQaH8qF0Rc'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Still comparing the two methods, this time using &lt;a href="https://unsplash.com/photos/dcasj22jmCk"&gt;this photo&lt;/a&gt; as input. We apply the normal maps and add some lights (rendered with Blender's realtime engine Eevee). Second row is without color to better visualize the normal maps :&lt;/p&gt;
&lt;p class='img_container'&gt;
    &lt;a href='https://hugotini.github.io/assets/deepbump/clay_compare.jpg'&gt;
        &lt;img class='article_img' src='https://hugotini.github.io/assets/deepbump/clay_compare.jpg'&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/dcasj22jmCk'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Another example with moving lights, without (left) / with (right) the generated normal map :&lt;/p&gt;
&lt;video controls&gt;
    &lt;source src='https://hugotini.github.io/assets/deepbump/brickdoor.webm'
            type="video/webm"&gt;
&lt;/video&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://unsplash.com/photos/ng-jV5Etz3c'&gt;original photo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In some cases, it might even work decently for some hand-painted textures :&lt;/p&gt;
&lt;video controls&gt;
    &lt;source src='https://hugotini.github.io/assets/deepbump/handpainted.webm'
            type="video/webm"&gt;
&lt;/video&gt;

&lt;p class='img_caption'&gt;&lt;a href='https://opengameart.org/content/handpainted-stone-wall-textures'&gt;original texture&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Limitations&lt;/h1&gt;
&lt;p&gt;Despite generalization techniques, machine learning models still depend on their training data. If the input picture is very different from what the neural net was trained on, risks are the output won't be ideal, but in any case, it's just one click so it costs nothing to give it a try.&lt;/p&gt;
&lt;h1&gt;Thanks&lt;/h1&gt;
&lt;p&gt;Many thanks to the authors of &lt;a href="https://texturehaven.com/"&gt;texturehaven.com&lt;/a&gt; and &lt;a href="https://cc0textures.com/"&gt;cc0textures.com&lt;/a&gt; and to their patreons for making such high quality assets available to all without restrictions. DeepBump's training dataset is based on those. Thanks to &lt;a href="https://github.com/qubvel/segmentation_models.pytorch"&gt;segmentation_models&lt;/a&gt; for making it easy to experiment with different network architectures in PyTorch.&lt;/p&gt;</content><category term="misc"></category></entry></feed>